{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.1"
      ],
      "metadata": {
        "id": "HxmlSQ41Xfve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n",
        "These are all the imports you will need for exercise 3.1. All exercises should be implemented using only the libraries below."
      ],
      "metadata": {
        "id": "xXQO_cTsXU4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB8_GkLAPjh9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EWLie0iU4Fo"
      },
      "source": [
        "## Linear Regression\n",
        "In this exercise, you will work on linear regression with polynomial features where we model the function $f(\\mathbf{x})$ as \n",
        "\n",
        "$$f(\\mathbf{x}) = \\mathbf{w}^\\intercal \\phi(\\mathbf{x}).$$\n",
        "\n",
        "The true model is a polynomial of degree 3\n",
        "\n",
        "$$f(x) = 0.5 + (2x - 0.5)^2 + x^3$$\n",
        "\n",
        "We further introduce noise into the system by adding a noise term $\\varepsilon_i$ which is sampled from a Gaussian distribution\n",
        "\n",
        "$$y = f(x) + \\varepsilon_i, \\varepsilon_i\\sim \\mathcal{N}(\\varepsilon; 0, \\sigma^2).$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9FMNGdKVQki"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  \"\"\"The true polynomial that generated the data D\n",
        "  Args:\n",
        "    x: Input data\n",
        "  Returns:\n",
        "    Polynomial evaluated at x \n",
        "  \"\"\"\n",
        "  return x ** 3 + (2 * x - .5) ** 2 + .5\n",
        "\n",
        "def generate_data(n, minval, maxval, variance=1., train=False, seed=0):\n",
        "  \"\"\"Generate the datasets. Note that we don't want to extrapolate, \n",
        "  and such, the eval data should always lie inside of the train data.\n",
        "  Args:\n",
        "    n: Number of datapoints to sample. n has to be atleast 2.\n",
        "    minval: Lower boundary for samples x\n",
        "    maxval: Upper boundary for samples x\n",
        "    variance: Variance or squared standard deviation of the model\n",
        "    train: Flag deciding whether we sample training or evaluation data\n",
        "    seed: Random seed\n",
        "  Returns:\n",
        "    Tuple of randomly generated data x and the according y\n",
        "  \"\"\"\n",
        "  # Set numpy random number generator\n",
        "  rng = np.random.default_rng(seed)\n",
        "\n",
        "  # Sample data along the x-axis\n",
        "  if train:\n",
        "    # We first sample uniformly on the x-Axis\n",
        "    x = rng.uniform(minval, maxval, size=(n - 2,))\n",
        "    # We will sample on datapoint beyond the min and max values to \n",
        "    # guarantee that we do not extrapolate during the evaluation\n",
        "    margin = (maxval - minval) / 100\n",
        "    min_x = rng.uniform(minval - margin, minval, (1,))\n",
        "    max_x = rng.uniform(maxval, maxval + margin, (1,))\n",
        "    x = np.concatenate((x, min_x, max_x))\n",
        "  else:\n",
        "    x = rng.uniform(minval, maxval, size=(n,))\n",
        "  eps = rng.standard_normal(n)\n",
        "\n",
        "  # \n",
        "  return x, f(x) + variance * eps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Least Squares Regression\n",
        "In this exercise we will study linear least squares regression with polynomial features. In particular, we want to evaluate the influence of the polynomial degree $k$ that we assume a priori."
      ],
      "metadata": {
        "id": "fQa1yhw1gOn0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn65pJCkYlM-"
      },
      "source": [
        "### Exercise 3.1.1\n",
        "To carry out regression, we first need to define the basis functions $\\phi(\\mathbf{x})$. In this task we would like to use polynomial features of degree $k$.\n",
        "\n",
        "Please work through the code and fill in the the `# TODO`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d-x2dg0YptP"
      },
      "outputs": [],
      "source": [
        "def polynomial_features(x, degree):\n",
        "  \"\"\"\n",
        "  Calculates polynomial features function of degree n.\n",
        "  The feature function includes all exponents from 0 to n.\n",
        "  Args:\n",
        "    x: Input of size (N, D)\n",
        "    degree: Polynomial degree\n",
        "  Returns:\n",
        "    Polynomial features evaluated at x of dim (degree, N)\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "\n",
        "def fit_w(x, y, lam, degree):\n",
        "  \"\"\"\n",
        "  Fit the weights with the closed-form solution of ridge regression.\n",
        "  Args:\n",
        "    x: Input of size (N, D)\n",
        "    y: Output of size (N,)\n",
        "    lam: Regularization parameter lambda\n",
        "    degree: Polynomial degree\n",
        "  Returns:\n",
        "    Optimal weights\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "\n",
        "def predict(x, w, degree):\n",
        "  \"\"\"\n",
        "  Calculate the generalized linear regression estimate given x, \n",
        "  the feature function, and weights w.\n",
        "  Args:\n",
        "    x: input of size (N, D)\n",
        "    w: Weights of size (M)\n",
        "    degree: Polynomial degree\n",
        "  Returns:\n",
        "    Generalized linear regression estimate\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "\n",
        "def calc_mse(x, y):\n",
        "  \"\"\"\n",
        "  Calculates the mean squared error (MSE) between x and y\n",
        "  Args:\n",
        "    x: Data x of size (N,)\n",
        "    y: Data y of size (N,)\n",
        "  Returns:\n",
        "    MSE \n",
        "  \"\"\"\n",
        "  # TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can try out your code by simply running the following cell. This cell will carry out your ridge regression implementation from above for $\\lambda=0$ in which case we are provided with the linear least squares solution.\n",
        "\n",
        "We evaluate the regression task on six different polynomial sizes $k = \\{0,1,3,5,7,9\\}$ based on your implementation of the MSE."
      ],
      "metadata": {
        "id": "V2D5Mri3SiMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Settings\n",
        "n_train = 15\n",
        "n_test = 100\n",
        "minval = -2.\n",
        "maxval = 2\n",
        "\n",
        "\n",
        "train_data = generate_data(n_train, minval, maxval, train=True, seed=1001)\n",
        "test_data = generate_data(n_test, minval, maxval, train=False, seed=1002)\n",
        "\n",
        "def plot_linear_regression(x, y, labels, eval_quantity):\n",
        "  \"\"\"Plotting functionality for the prediction of linear regression \n",
        "  for K different polynomial degrees.\n",
        "  Args:\n",
        "    x: Data of size (K, N). The first dimension denotes the different \n",
        "      polynomial degrees that has been experimented with\n",
        "    y: Data of size (K, N)\n",
        "  \"\"\"\n",
        "  K = x.shape[0]\n",
        "  colors = mpl.colormaps['Reds'].resampled(K+1)(range(1, K+1))\n",
        "  fig = plt.figure()\n",
        "  plt.scatter(test_data[0], test_data[1], color=\"tab:orange\", linewidths=0.5, label=\"Test\", alpha=0.3)\n",
        "  plt.scatter(train_data[0], train_data[1], color=\"tab:blue\", linewidths=0.5, label=\"Train\", alpha=0.3)\n",
        "  for i in range(K):\n",
        "    plt.plot(x[i], y[i], label=f\"{eval_quantity}={labels[i]}\", color=colors[i], lw=2.5)\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.legend()\n",
        "\n",
        "def plot_mse(mse, labels):\n",
        "  \"\"\"Plotting functionality of the MSE for K different polynomial degrees.\"\"\"\n",
        "  fig = plt.figure()\n",
        "  plt.plot(labels, mse)\n",
        "  plt.scatter(labels, mse)\n",
        "  plt.xticks(labels)\n",
        "  plt.ylabel(\"MSE\")\n",
        "  plt.xlabel(\"Polynomial Degree\")\n",
        "\n",
        "# Evaluate regression for different polynomial degrees\n",
        "degrees = [0, 1, 3, 5, 7, 9]\n",
        "xs, ys, mse = [], [], []\n",
        "for degree in degrees:\n",
        "  w = fit_w(\n",
        "      train_data[0], train_data[1], \n",
        "      lam=0., # Edge case resulting in linear least squares regression\n",
        "      degree=degree\n",
        "  )\n",
        "  # Predict the test data\n",
        "  y_test = predict(test_data[0], w, degree)\n",
        "  mse.append(calc_mse(y_test, test_data[1]))\n",
        "  \n",
        "  # Run regression over the whole interval\n",
        "  xs.append(np.linspace(minval, maxval, 100))\n",
        "  ys.append(predict(xs[-1], w, degree))\n",
        "xs = np.stack(xs)\n",
        "ys = np.stack(ys)\n",
        "\n",
        "plot_linear_regression(xs, ys, labels=degrees, eval_quantity=\"k\")\n",
        "plot_mse(mse, degrees)"
      ],
      "metadata": {
        "id": "1ClSpJsbdgSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1.2\n",
        "Please describe your results below in a few lines thereby answering which model you would choose and which phenomenon we see for small and large polynomial degrees.\n",
        "\n",
        "---\n",
        "\n",
        "`TODO: Your answer here`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cSt0OVXlTfoq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN0_fsz4QckH"
      },
      "source": [
        "## Bias Variance Tradeoff\n",
        "Next up, we will compare the model performance of **ridge regression** based on the penalty parameter $\\lambda$. For that we will evaluate the expected squared error of the true model against our predictions. As we have shown in the lecture, this leads to the bias-variance decomposition\n",
        "\n",
        "$$L_{\\hat{f}}(\\mathbf{x}_{q}) = \\mathbb{E}_{\\mathcal{D}, \\varepsilon}\\left[\\bigr(y(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\bigl)^{2}\\right] = \\sigma^2 + \\textrm{bias}^{2}\\left[ \\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\right] + \\textrm{var}\\left[ \\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\right]$$\n",
        "\n",
        "Here $\\hat{f}_{\\mathcal{D}}$ denotes the function estimator trained on the data $\\mathcal{D} = \\{(y_i, \\mathbf{x_i})\\mid i=1,\\dots,N\\}$.\n",
        "We have left the two following identities open in the lecture which are required to arrive at the above equation\n",
        "\\begin{align*}\n",
        "    \\mathbb{E}_{\\mathcal{D},\\varepsilon}\\left[\\varepsilon\\thinspace\\big(f(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x_q})\\big)\\right] &= 0\\\\\n",
        "    \\mathbb{E}_{\\mathcal{D}}\\left[\\bigr(f(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\bigl)^{2}\\right] &= \\bigr(f(\\mathbf{x}_{q})-\\bar{\\hat{f}}(\\mathbf{x}_{q})\\bigl)^{2} + \\mathbb{E}_{\\mathcal{D}}\\left[\\bigr(\\bar{\\hat{f}}(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\bigl)^{2}\\right]\n",
        "\\end{align*}\n",
        "Here, the notation is simplified by adding the variable $\\bar{\\hat{f}}(\\mathbf{x}_{q})=\\mathbb{E}_{\\mathcal{D}}\\left[\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\right]$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1.3\n",
        "Please show the two identities \n",
        "1. $\\mathbb{E}_{\\mathcal{D},\\varepsilon}\\left[\\varepsilon\\thinspace\\big(f(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x_q})\\big)\\right] = 0$\n",
        "2. $\\mathbb{E}_{\\mathcal{D}}\\left[\\bigr(f(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\bigl)^{2}\\right] = \\bigr(f(\\mathbf{x}_{q})-\\bar{\\hat{f}}(\\mathbf{x}_{q})\\bigl)^{2} + \\mathbb{E}_{\\mathcal{D}}\\left[\\bigr(\\bar{\\hat{f}}(\\mathbf{x}_{q})-\\hat{f}_{\\mathcal{D}}(\\mathbf{x}_{q})\\bigl)^{2}\\right]$\n",
        "\n",
        "---\n",
        "`TODO: Your answer here`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EENw2Xn_c_l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bias-variance tradeoff is typically a purely theoretical concept as it requires the evaluation of $f(x)$. In this task we assume that $f(x)$ is known and thus, an approximation of the bias and variance is possible. We approximatie the bias and variance by its sample means \n",
        "$$\\text{Bias}\\,\\text{bias}^2[\\hat{f}_\\mathcal{D}] \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\left(f(x_i) - \\bar{\\hat{f}}(x_i)\\right),$$\n",
        "\n",
        "$$\\text{Var}\\,\\text{var}\\left[\\hat{f}_\\mathcal{D}\\right] \\approx \\frac{1}{NM}\\sum_{i=1}^{N}\\sum_{j=1}^{M}\\left(\\hat{f}_{\\mathcal{D}_j}(x_i) - \\bar{\\hat{f}}(x_i)\\right)^2$$\n",
        "\n",
        "Here, $\\bar{\\hat{f}}(x_i)$ is the average prediction of the maximum likelihood over the data distribution $p(\\mathcal{D})$ which we approximate given $M$ datasets $\\mathcal{D}_j$\n",
        "$$\\bar{\\hat{f}}(x_i) \\approx \\frac{1}{M}\\sum_{j=1}^{M}\\left(f_{\\mathcal{D_j}}(x_i)\\right).$$\n",
        "\n",
        "To approximate the bias and variance, we first evaluate the maximum likelihood estimate $f_{\\mathcal{D}_j}$ for each dataset $\\mathcal{D}_j$. Afterwards we can approximate the two terms."
      ],
      "metadata": {
        "id": "GCYEuTTtcJvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1.4\n",
        "In this exercise we implement the average prediction $\\bar{\\hat{f}}(x_i)$, $\\text{Bias}\\,\\text{bias}^2[\\hat{f}_\\mathcal{D}]$, and $\\text{Var}\\,\\text{var}\\left[\\hat{f}_\\mathcal{D}\\right]$ as introduced above. \n",
        "\n",
        "Please work through the code and fill in the the `# TODO`s."
      ],
      "metadata": {
        "id": "3pEEHk00XjfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3j_KN06z7df"
      },
      "outputs": [],
      "source": [
        "def avg_prediction(x, ws, degree=3):\n",
        "  \"\"\"\n",
        "  Approximation of the average prediction using the M function estimations\n",
        "  Args:\n",
        "    x: input data of size (N,)\n",
        "    ws: The weights obtained from ridge regression of size (M, degree)\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    The average prediction as a scalar\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "\n",
        "\n",
        "def calc_bias(x_q, ws, degree):\n",
        "  \"\"\"Estimate the bias.\n",
        "  Args:\n",
        "    x_q: Queries x of size (N,)\n",
        "    ws: The weights obtained from ridge regression of size (M, degree)\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    Bias\n",
        "  \"\"\"\n",
        "  # TODO: Your code here\n",
        "\n",
        "def calc_variance(x_q, ws, degree):\n",
        "  \"\"\"Estimate the model variance\n",
        "  Args:\n",
        "    x_q: Queries x of size (N,)\n",
        "    ws: The weights obtained from ridge regression of size (M, degree)\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    Model variance\n",
        "  \"\"\"\n",
        "  # TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test your implementation by running the below coding snippet. It estimate the bias and variance for $M = 25$ datasets with each dataset containing $N=20$ datapoints."
      ],
      "metadata": {
        "id": "qw25sGsiZBHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jjfZ-HM06cS"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Settings\n",
        "n = 20\n",
        "m = 25\n",
        "degree = 9\n",
        "train_datasets = []\n",
        "seed = 3001\n",
        "for i in range(m):\n",
        "  train_datasets.append(generate_data(n_train, minval, maxval, train=True, seed=seed))\n",
        "  seed += 1\n",
        "eval_points = np.linspace(minval, maxval, n)\n",
        "\n",
        "# Estimate the bias and variance\n",
        "biases = []\n",
        "vars = []\n",
        "xs, ys = [], []\n",
        "lambdas = [0.0001, 0.001, 0.01, 0.1, 1., 10]\n",
        "for l in lambdas:\n",
        "  w_maps = []\n",
        "  for data in train_datasets:\n",
        "    w = fit_w(\n",
        "      data[0], data[1],\n",
        "      l,\n",
        "      9\n",
        "    )\n",
        "    w_maps.append(w)\n",
        "  bias = calc_bias(eval_points, w_maps, degree)\n",
        "  biases.append(bias)\n",
        "  var = calc_variance(eval_points, w_maps, degree)\n",
        "  vars.append(var)\n",
        "  xs.append(np.linspace(minval, maxval, 100))\n",
        "  ys.append(predict(xs[-1], w_maps[0], degree))\n",
        "\n",
        "biases = np.array(biases)\n",
        "vars = np.array(vars)\n",
        "xs = np.stack(xs)\n",
        "ys = np.stack(ys)\n",
        "\n",
        "# Plot the bias and variance for different lambas\n",
        "plt.figure()\n",
        "plt.plot(lambdas, biases, label=\"Bias\")\n",
        "plt.plot(lambdas, vars, label=\"Variance\")\n",
        "plt.plot(lambdas, biases + vars, label=\"Total Err.\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.legend()\n",
        "\n",
        "# Calculate predictions\n",
        "plot_linear_regression(xs, ys, labels=lambdas, eval_quantity=r\"$\\lambda$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1.5\n",
        "Please explain the results in a few sentences. In particular, provide an explanation if the bias and variance behave as expected. For which regularization parameter $\\lambda$ would you decide?\n",
        "\n",
        "---\n",
        "`TODO: Your answer here`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xTXKIC8OZZOX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtabnR5-pkFi"
      },
      "source": [
        "## Gradient Descent\n",
        "In the lecture we have seen that the closed form solution of linear regression requires us to take the inverse $(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi})^{-1}$. For high dimensional features, the inverse can be a high computational burden. For these reasons, gradient descent provides an alternative to approximate the weight vector."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1.6\n",
        "Please implement gradient descent optimization to find the regression weights $\\mathbf{w}$. We will use the loss from linear least squares with polynomial features of degree $k=3$\n",
        "\n",
        "$$\\mathcal{J}(\\mathbf{w}) = ||\\boldsymbol{\\Phi}^\\intercal\\mathbf{w} - \\mathbf{y}||^2.$$\n",
        "\n",
        "The number of gradient updates is fixed to $n_{\\text{iter}} = 1000$. The learning rate can be freely chosen, but a good initial value is lr=0.0001. Please update the gradient by using all the training data points $n_\\text{train}$, i.e., no mini-batches.\n",
        "\n",
        "We expect you to provide a plot of the learning curve, i.e., a plot of the MSE on the test data against the iterations. You can evaluate your model after $n_{\\text{eval}}=20$ gradient updates. We further would like to see the model prediction after $n=0, 10, 100, 1000$ gradient updates/iterations.\n",
        "\n",
        "In this task we expect you to provide the full code. Note that you are allowed to use all functions defined above."
      ],
      "metadata": {
        "id": "ld6Mnvbshf1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju71bo4apm_Q"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Settings\n",
        "n_train = 15\n",
        "n_test = 100\n",
        "minval = -2.\n",
        "maxval = 2\n",
        "degree = 3\n",
        "\n",
        "\n",
        "train_data = generate_data(n_train, minval, maxval, train=True, seed=4001)\n",
        "test_data = generate_data(n_test, minval, maxval, train=False, seed=4002)\n",
        "\n",
        "# TODO: Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}