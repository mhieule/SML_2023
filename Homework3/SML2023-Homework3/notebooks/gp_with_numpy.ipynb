{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.2"
      ],
      "metadata": {
        "id": "FCxng_0We8eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility function for nicely plotting GPs"
      ],
      "metadata": {
        "id": "2W0rigv3K5_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_gp(mu, cov, X, X_train=None, Y_train=None, samples=[]):\n",
        "    '''\n",
        "    This function plots 95% confidence interval of GP given its parameters. It\n",
        "    also plots data points and function samples if provided.\n",
        "    \n",
        "    Args:\n",
        "        mu: mean values (n, d).\n",
        "        cov: kernel matrix (n, n).\n",
        "        X_train: training data points (m, d).\n",
        "        Y_train: training data labels (m, 1).\n",
        "        samples: list of function samples ([n, d]).\n",
        "    '''\n",
        "    X = X.ravel()\n",
        "    mu = mu.ravel()\n",
        "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\n",
        "    \n",
        "    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.1)\n",
        "    plt.plot(X, mu, label='Mean')\n",
        "    for i, sample in enumerate(samples):\n",
        "        plt.plot(X, sample, lw=1, ls='--', label=f'Sample {i+1}')\n",
        "    if X_train is not None:\n",
        "        plt.plot(X_train, Y_train, 'rx')\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "B7-fd-TvK-U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(11111)  # fixed seed for grading"
      ],
      "metadata": {
        "id": "-LmrU96jvox_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ghz1khBJCrK"
      },
      "source": [
        "## Gaussian Process Implementation with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuOOMHIdJCrK"
      },
      "source": [
        "In this task, we will use the squared exponential kernel, also known as Gaussian kernel or RBF kernel:\n",
        "\n",
        "$$\n",
        "\\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp(-\\frac{1}{2l^2}\n",
        "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
        "  (\\mathbf{x}_i - \\mathbf{x}_j))\n",
        "$$\n",
        "\n",
        "The length parameter $l$ controls the smoothness of the function and $\\sigma_f$ the vertical variation. For simplicity, we use the same length parameter $l$ for all input dimensions (isotropic kernel). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.2.1 \n",
        "Implement the isotropic Gaussian kernel in the function below."
      ],
      "metadata": {
        "id": "OU1OH01wdAcC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IloaniasJCrK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
        "    '''\n",
        "    Isotropic squared exponential kernel. Computes \n",
        "    a covariance matrix from points in X1 and X2.\n",
        "    \n",
        "    Args:\n",
        "        X1: Array of m points (m, d).\n",
        "        X2: Array of n points (n, d).\n",
        "\n",
        "    Returns:\n",
        "        Covariance matrix (m, n).\n",
        "    '''\n",
        "    # TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d86uTD3pJCrL"
      },
      "source": [
        "There are many other kernels for Gaussian processes. For example, please see the scikit-learn documentation for [some kernel examples](http://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015xT-6OJCrL"
      },
      "source": [
        "## Prior\n",
        "\n",
        "Let us first define a prior over functions with mean zero and a covariance matrix computed with kernel parameters $l=1$ and $\\sigma_f=1$. Now, to see the random functions that can be sampled from this GP, we draw random samples from the corresponding multivariate normal."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.2.2\n",
        "Draw four random samples and plots it together with the zero mean and the 95% confidence interval (computed from the diagonal of the covariance matrix).\n",
        "\n",
        "**Hint:** use `np.random.multivariate_normal`. "
      ],
      "metadata": {
        "id": "hK1qSnMreVri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEKts9nQJCrL"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Finite number of points\n",
        "X = np.arange(-5, 5, 0.2).reshape(-1, 1)\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9n5uHYSJCrM"
      },
      "source": [
        "## Prediction from training data\n",
        "\n",
        "In the following tasks, we will learn the `sine` function with GP and do regression tasks for data generated from both noise-free case\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{x}) = \\text{sin}(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "and noisy case\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{x}) = \\text{sin}(\\mathbf{x}) + \\mathbf{\\epsilon},\\, \\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathbf{I}).\n",
        "$$\n",
        "\n",
        "\n",
        "In the lecture, we have seen shown how to compute the sufficient statistics, i.e., the mean and the covariance of the posterior predictive distribution for a new data point. Now, in this task, we generalize the posterior predictive distribution for multiple new data points.\n",
        "\n",
        "A GP prior $p(\\mathbf{f} \\lvert \\mathbf{X})$ can be converted into a GP posterior $p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})$ after having observed some data $\\mathbf{y}$. The posterior can then be used to make predictions $\\mathbf{f}_*$ given new input $\\mathbf{X}_*$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) \n",
        "&= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ \n",
        "&= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "By definition of the GP, the joint distribution of observed data $\\mathbf{y}$ and predictions $\\mathbf{f}_*$ is\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{pmatrix} \\sim \\mathcal{N}\n",
        "\\left(\\boldsymbol{0},\n",
        "\\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{pmatrix}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "With $N$ training data and $N_*$ new input data, $\\mathbf{K}_y = \\kappa(\\mathbf{X},\\mathbf{X}) + \\sigma_y^2\\mathbf{I} = \\mathbf{K} + \\sigma_y^2\\mathbf{I}$ is $N \\times N$, $\\mathbf{K}_* = \\kappa(\\mathbf{X},\\mathbf{X}_*)$ is $N \\times N_*$ and $\\mathbf{K}_{**} = \\kappa(\\mathbf{X}_*,\\mathbf{X}_*)$ is $N_* \\times N_*$. $\\sigma_y^2$ is the noise term in the diagonal of $\\mathbf{K_y}$. It is set to zero if training targets are noise-free and to a value greater than zero if training observations are noisy. The mean is set to $\\boldsymbol{0}$ for notational simplicity. The sufficient statistics of the posterior predictive distribution, $\\boldsymbol{\\mu}_*$ and $\\boldsymbol{\\Sigma}_*$, can be computed by\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y} \\\\\n",
        "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*\n",
        "\\end{align*}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Exercise 3.2.3 \n",
        " Implement the below function that computes the predictive mean and variances. Apply them to both noise-free and noisy training data `X_train` and `Y_train`. Draw four samples from the predictive posterior and plot them along with the mean, confidence interval and training data. Why don't the samples go through the training data points in noisy case?"
      ],
      "metadata": {
        "id": "7pkte_tMpnJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQAFckNpJCrM"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import inv\n",
        "\n",
        "def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
        "    '''\n",
        "    Computes the suffifient statistics of the GP posterior predictive distribution \n",
        "    from m training data X_train and Y_train and n new inputs X_s.\n",
        "    \n",
        "    Args:\n",
        "        X_s: New input locations (n, d).\n",
        "        X_train: Training locations (m, d).\n",
        "        Y_train: Training targets (m, 1).\n",
        "        l: Kernel length parameter.\n",
        "        sigma_f: Kernel vertical variation parameter.\n",
        "        sigma_y: Noise parameter.\n",
        "    \n",
        "    Returns:\n",
        "        Posterior mean vector (n, d) and covariance matrix (n, n).\n",
        "    '''\n",
        "    # TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCncC-y_JCrM"
      },
      "outputs": [],
      "source": [
        "# Noise free training data\n",
        "X_train = np.arange(-3, 4, 1).reshape(-1, 1)\n",
        "Y_train = np.sin(X_train)\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj9r7s_wJCrN"
      },
      "outputs": [],
      "source": [
        "noise = 0.4\n",
        "\n",
        "# Noisy training data\n",
        "X_train = np.arange(-3, 4, 1).reshape(-1, 1)\n",
        "Y_train = np.sin(X_train) + noise * np.random.randn(*X_train.shape)\n",
        "\n",
        "# TODO: Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "`TODO: Your answer here`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rnItkgEuq6NV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMGb_ZNaJCrN"
      },
      "source": [
        "## Effect of kernel parameters and noise parameters\n",
        "\n",
        "The following task shows the effect of kernel parameters $l$ and $\\sigma_f$ as well as the noise parameter $\\sigma_y$. Higher $l$ values lead to smoother functions and therefore to coarser approximations of the training data. Lower $l$ values make functions more wiggly with wide confidence intervals between training data points. $\\sigma_f$ controls the vertical variation of functions drawn from the GP. This can be seen by the wide confidence intervals outside the training data region in the right figure of the second row. $\\sigma_y$ represents the amount of noise in the training data. Higher $\\sigma_y$ values make more coarse approximations which avoids overfitting to noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O77ci-IjJCrN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "params = [\n",
        "    (0.3, 1.0, 0.2),\n",
        "    (3.0, 1.0, 0.2),\n",
        "    (1.0, 0.3, 0.2),\n",
        "    (1.0, 3.0, 0.2),\n",
        "    (1.0, 1.0, 0.05),\n",
        "    (1.0, 1.0, 1.5),\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "for i, (l, sigma_f, sigma_y) in enumerate(params):\n",
        "    mu_s, cov_s = posterior_predictive(X, X_train, Y_train, l=l, \n",
        "                                       sigma_f=sigma_f, \n",
        "                                       sigma_y=sigma_y)\n",
        "    plt.subplot(3, 2, i + 1)\n",
        "    plt.subplots_adjust(top=2)\n",
        "    plt.title(f'l = {l}, sigma_f = {sigma_f}, sigma_y = {sigma_y}')\n",
        "    plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqb2giTjJCrN"
      },
      "source": [
        "Optimal values for these parameters can be estimated by maximizing the log marginal likelihood\n",
        "\n",
        "$$\n",
        "\\log p(\\mathbf{y} \\lvert \\mathbf{X}) = \n",
        "\\log \\mathcal{N}(\\mathbf{y} \\lvert \\boldsymbol{0},\\mathbf{K}_y) =\n",
        "-\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}_y^{-1} \\mathbf{y} \n",
        "-\\frac{1}{2} \\log \\begin{vmatrix}\\mathbf{K}_y\\end{vmatrix} \n",
        "-\\frac{N}{2} \\log(2\\pi)\n",
        "$$\n",
        "\n",
        "In this task, we will minimize the negative log marginal likelihood w.r.t. parameters $l$ and $\\sigma_f$, $\\sigma_y$ is set to the known noise level of the data. If the noise level is unknown, $\\sigma_y$ can be estimated as well along with the other parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Exercise 3.2.4 \n",
        " Implement the log likehood objective below. Then, given the noisy training data above, optimize for the parameters $\\mathbf{\\theta} = [l, \\sigma_f, \\sigma_y]$ w.r.t. the log likelihood objective. Finally, compute the predictive mean and variances with the optimized parameters and plot the result GP together with the training data.\n",
        "\n",
        "**Hint:** see `scipy.optimize.minimize` [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)."
      ],
      "metadata": {
        "id": "8u_t_aX9sR4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye_EAgU8JCrN"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "def nll_fn(X_train, Y_train, noise):\n",
        "    '''\n",
        "    Returns a function that computes the negative log marginal\n",
        "    likelihood for training data X_train and Y_train and given \n",
        "    noise level.\n",
        "    \n",
        "    Args:\n",
        "        X_train: training locations (m, d).\n",
        "        Y_train: training targets (m, 1).\n",
        "        noise: known noise level of Y_train.\n",
        "\n",
        "    Returns:\n",
        "        Minimization objective.\n",
        "    '''\n",
        "    def nll(theta):\n",
        "        # TODO: Your code here\n",
        "        return # objective value\n",
        "    return nll\n",
        "\n",
        "# use scipy.optimize.minimize to minimize the parameters theta\n",
        "# TODO: Your code here \n",
        "\n",
        "# compute the prosterior predictive statistics with optimized kernel parameters and plot the results\n",
        "# TODO: Your code here\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}